#!/usr/bin/env python3
"""
å¾tcm_sd_test_rc_direct.jsonä¸­åŠ è¼‰æ¸¬è©¦æ•¸æ“šé€²è¡Œæª¢ç´¢è©•ä¼°
ä½¿ç”¨æ‚£è€…è‡¨åºŠä¿¡æ¯ä½œç‚ºæŸ¥è©¢ï¼Œè©•ä¼°æª¢ç´¢ç³»çµ±çš„æ€§èƒ½
ä½¿ç”¨OpenAI text-embedding-3-largeè¨ˆç®—èªæ„ç›¸ä¼¼åº¦ä¸¦è¨ˆç®—RR (Reciprocal Rank)
"""

import os
import sys
import json
import random
import logging
import argparse
import asyncio
import requests
import time
from typing import List, Dict, Any, Tuple, Optional
from dotenv import load_dotenv
from langchain_openai import OpenAIEmbeddings
from langchain_community.vectorstores import Chroma
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain.embeddings.base import Embeddings
import matplotlib.pyplot as plt
import numpy as np
from tqdm.asyncio import tqdm
from tqdm import tqdm as sync_tqdm

# é…ç½®æ—¥èªŒ
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logging.getLogger("openai").setLevel(logging.WARNING)
logging.getLogger("httpx").setLevel(logging.WARNING)

# åŠ è¼‰ç’°å¢ƒè®Šé‡
load_dotenv()

# Supported embedding providers and models (same as ingest.py)
EMBEDDING_PROVIDERS = {
    "openai": {
        "models": [
            "text-embedding-3-large"
        ],
        "default": "text-embedding-3-large"
    },
    "vllm": {
        "models": [
            "Qwen3-Embedding-8B"
        ],
        "default": "Qwen3-Embedding-8B",
        "base_url": "http://localhost:8010/v1"
    },
    "custom": {
        "models": [
            "Qwen3-Embedding-0.6B-base",
            "Qwen3-Embedding-0.6B-finetuned",
            "Qwen3-Embedding-4B-base",
            "Qwen3-Embedding-4B-finetuned"
        ],
        "default": "Qwen3-Embedding-0.6B-finetuned",
        "base_url": "http://localhost:8000/v1"
    },
    "huggingface": {
        "models": [
            "BAAI/bge-large-zh-v1.5"
        ],
        "default": "BAAI/bge-large-zh-v1.5"
    }
}

class CustomEmbeddings(Embeddings):
    """Custom embeddings class that works with your fine-tuned model API."""
    
    def __init__(self, base_url: str, model: str):
        self.base_url = base_url
        self.model = model
        self.endpoint = f"{base_url}/embeddings"
    
    def embed_documents(self, texts: List[str]) -> List[List[float]]:
        """Embed multiple documents."""
        embeddings = []
        for text in texts:
            response = requests.post(
                self.endpoint,
                json={"model": self.model, "input": text}
            )
            response.raise_for_status()
            embedding = response.json()["data"][0]["embedding"]
            embeddings.append(embedding)
        return embeddings
    
    def embed_query(self, text: str) -> List[float]:
        """Embed a single query."""
        response = requests.post(
            self.endpoint,
            json={"model": self.model, "input": text}
        )
        response.raise_for_status()
        return response.json()["data"][0]["embedding"]

def get_provider_and_model(embedding_model: str) -> tuple[str, str]:
    """Determine the provider and model name from the embedding_model string."""
    
    # Check if it's a provider:model format (e.g., "custom:Qwen3-Embedding-0.6B-finetuned")
    if ":" in embedding_model:
        provider, model = embedding_model.split(":", 1)
        if provider in EMBEDDING_PROVIDERS:
            if model in EMBEDDING_PROVIDERS[provider]["models"]:
                return provider, model
            else:
                logging.error(f"Model '{model}' not supported for provider '{provider}'")
                logging.info(f"Supported models for {provider}: {EMBEDDING_PROVIDERS[provider]['models']}")
                raise ValueError(f"Unsupported model '{model}' for provider '{provider}'")
        else:
            logging.error(f"Unsupported provider: {provider}")
            logging.info(f"Supported providers: {list(EMBEDDING_PROVIDERS.keys())}")
            raise ValueError(f"Unsupported provider: {provider}")
    
    # Check if it's a direct model name and find the matching provider
    for provider, config in EMBEDDING_PROVIDERS.items():
        if embedding_model in config["models"]:
            return provider, embedding_model
    
    # If not found, assume it's an OpenAI model (for backward compatibility)
    logging.warning(f"Model '{embedding_model}' not found in predefined models. Assuming OpenAI provider.")
    return "openai", embedding_model

def initialize_embeddings(embedding_model: str):
    """Initialize the appropriate embeddings class based on the model specification."""
    
    provider, model = get_provider_and_model(embedding_model)
    
    logging.info(f"Initializing {provider} embeddings with model: {model}")
    
    if provider == "openai":
        # Check if OPENAI_API_KEY is available
        if not os.getenv("OPENAI_API_KEY"):
            logging.error("OPENAI_API_KEY environment variable not set for OpenAI embeddings.")
            raise ValueError("OPENAI_API_KEY required for OpenAI embeddings")
        
        return OpenAIEmbeddings(model=model)
    
    elif provider == "vllm":
        # Use OpenAI-compatible client pointing to vLLM server
        base_url = EMBEDDING_PROVIDERS[provider]["base_url"]
        logging.info(f"Using vLLM server at: {base_url}")
        
        # Try to create OpenAI embeddings with different parameter names for compatibility
        try:
            # First try with newer langchain-openai version parameters
            return OpenAIEmbeddings(
                model=model,
                api_key="unused",  # vLLM doesn't require a real API key
                base_url=base_url
            )
        except TypeError:
            try:
                # Try with older parameter names
                return OpenAIEmbeddings(
                    model=model,
                    openai_api_key="unused",
                    openai_api_base=base_url
                )
            except Exception as e:
                logging.error(f"Failed to initialize vLLM embeddings: {e}")
                logging.info(f"Please ensure vLLM server is running at {base_url}")
                raise
    
    elif provider == "custom":
        # Use custom embeddings class for fine-tuned model server
        base_url = EMBEDDING_PROVIDERS[provider]["base_url"]
        logging.info(f"Using custom fine-tuned model server at: {base_url}")
        
        try:
            return CustomEmbeddings(base_url=base_url, model=model)
        except Exception as e:
            logging.error(f"Failed to initialize custom embeddings: {e}")
            logging.info(f"Please ensure custom server is running at {base_url}")
            raise
    
    elif provider == "huggingface":
        # Use local Hugging Face model
        logging.info(f"Loading Hugging Face model: {model}")
        
        return HuggingFaceEmbeddings(
            model_name=model,
            encode_kwargs={'normalize_embeddings': True}  # Normalize embeddings for better similarity search
        )
    
    else:
        raise ValueError(f"Unsupported embedding provider: {provider}")


class RetrievalEvaluator:
    """æª¢ç´¢è©•ä¼°å™¨"""
    
    def __init__(self, db_name: str = "syndrome_db", embedding_model: str = "text-embedding-3-large", max_concurrent: int = 5, 
                 use_rerank: bool = False, reranker_api_url: str = "http://localhost:8001", 
                 reranker_model: str = "Qwen/Qwen3-Reranker-0.6B", rerank_top_n: int = 100):
        self.db_name = db_name
        self.persist_directory = os.path.join("chroma_dbs", db_name)
        self.embedding_model = embedding_model
        self.max_concurrent = max_concurrent
        self.use_rerank = use_rerank
        self.reranker_api_url = reranker_api_url
        self.reranker_model = reranker_model
        self.rerank_top_n = rerank_top_n
        
        # æª¢æŸ¥æ•¸æ“šåº«æ˜¯å¦å­˜åœ¨
        if not os.path.exists(self.persist_directory):
            raise FileNotFoundError(f"âŒ æ•¸æ“šåº«ç›®éŒ„ä¸å­˜åœ¨: {self.persist_directory}")
        
        # åˆå§‹åŒ–embeddingså’Œvectorstore
        self.embeddings = initialize_embeddings(self.embedding_model)
        self.vectorstore = Chroma(
            persist_directory=self.persist_directory,
            embedding_function=self.embeddings
        )
        
        # å‰µå»ºä¿¡è™Ÿé‡ä¾†æ§åˆ¶ä½µç™¼
        self.semaphore = asyncio.Semaphore(max_concurrent)
        
        logging.info(f"âœ… å·²åˆå§‹åŒ–æ•¸æ“šåº«: {db_name}")
        logging.info(f"ğŸ” ä½¿ç”¨embeddingæ¨¡å‹: {embedding_model}")
        logging.info(f"âš¡ æœ€å¤§ä½µç™¼æ•¸: {max_concurrent}")
        if self.use_rerank:
            logging.info(f"ğŸ”„ å•Ÿç”¨å…©éšæ®µæª¢ç´¢ - Reranker API: {reranker_api_url}")
            logging.info(f"ğŸ¯ Rerankeræ¨¡å‹: {reranker_model}")
            logging.info(f"ğŸ“Š Rerank Top-N: {rerank_top_n}")
            # æª¢æŸ¥ Reranker API å¥åº·ç‹€æ…‹
            self._check_reranker_health()
    
    def _check_reranker_health(self):
        """æª¢æŸ¥ Reranker API å¥åº·ç‹€æ…‹"""
        try:
            response = requests.get(f"{self.reranker_api_url}/health", timeout=10)
            if response.status_code == 200:
                health_data = response.json()
                logging.info(f"âœ… Reranker API å¥åº·æª¢æŸ¥é€šé")
                logging.info(f"ğŸ“‹ APIç‹€æ…‹: {health_data.get('status', 'unknown')}")
                logging.info(f"ğŸ–¥ï¸  é‹è¡Œè¨­å‚™: {health_data.get('device', 'unknown')}")
                
                # æª¢æŸ¥æ¨¡å‹æ˜¯å¦åŒ¹é…
                api_model = health_data.get('model', '')
                if api_model and api_model != self.reranker_model:
                    logging.warning(f"âš ï¸ æ¨¡å‹ä¸åŒ¹é… - é…ç½®: {self.reranker_model}, API: {api_model}")
            else:
                logging.error(f"âŒ Reranker API å¥åº·æª¢æŸ¥å¤±æ•—: {response.status_code}")
        except requests.exceptions.ConnectionError:
            logging.error(f"âŒ ç„¡æ³•é€£æ¥åˆ° Reranker API: {self.reranker_api_url}")
            logging.error("è«‹ç¢ºä¿ Reranker API æœå‹™æ­£åœ¨é‹è¡Œ")
        except Exception as e:
            logging.warning(f"âš ï¸ Reranker API å¥åº·æª¢æŸ¥ç•°å¸¸: {str(e)}")
    
    def load_test_data(self, json_file_path: str) -> List[Dict[str, Any]]:
        """åŠ è¼‰æ¸¬è©¦æ•¸æ“š"""
        try:
            with open(json_file_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
            logging.info(f"ğŸ“Š å·²åŠ è¼‰ {len(data)} æ¢æ¸¬è©¦æ•¸æ“š")
            return data
        except Exception as e:
            logging.error(f"âŒ åŠ è¼‰æ•¸æ“šå¤±æ•—: {e}")
            raise
    
    def select_random_queries(self, data: List[Dict[str, Any]], num_queries: int = 10) -> List[Dict[str, Any]]:
        """éš¨æ©Ÿé¸æ“‡æ¸¬è©¦é …ç›®"""
        if len(data) < num_queries:
            logging.warning(f"âš ï¸  æ•¸æ“šé‡ ({len(data)}) å°‘æ–¼è«‹æ±‚æ•¸é‡ ({num_queries})")
            num_queries = len(data)
        
        selected = random.sample(data, num_queries)
        logging.info(f"ğŸ² å·²éš¨æ©Ÿé¸æ“‡ {num_queries} å€‹æ¸¬è©¦é …ç›®")
        return selected
    
    def generate_query_text(self, item: Dict[str, Any], query_type: str = "prompt", use_pseudo_doc: bool = False, use_keywords: bool = False) -> str:
        """å¾æ¸¬è©¦é …ç›®ä¸­æå–æŸ¥è©¢æ–‡æœ¬"""
        if query_type == "prompt":
            # å¦‚æœå•Ÿç”¨keywordsæª¢ç´¢ä¸”å­˜åœ¨keywordså­—æ®µ
            if use_keywords and "keywords" in item:
                keywords = item.get("keywords", "").strip()
                if keywords:
                    return keywords
            
            query_text = item.get("prompt", "").strip()
            
            # å¦‚æœå•Ÿç”¨Query2Docæ–¹æ³•ä¸”å­˜åœ¨pseudo_document
            if use_pseudo_doc and "pseudo_document" in item:
                pseudo_doc = item.get("pseudo_document", "").strip()
                if pseudo_doc:
                    # å°‡åŸå§‹æŸ¥è©¢èˆ‡pseudo_documentçµåˆ
                    query_text = f"{query_text}\n\n{pseudo_doc}"
            
            return query_text
        else:
            # ä¿æŒå‘å¾Œå…¼å®¹ï¼Œé›–ç„¶å°æ–°æ•¸æ“šæ ¼å¼å¯èƒ½ç„¡æ•ˆ
            return item.get(query_type, "").strip()
    
    def search_with_vectorstore(self, query: str, k: int = 10) -> List[Dict[str, Any]]:
        """ä½¿ç”¨vectorstoreé€²è¡Œç›¸ä¼¼åº¦æœç´¢ï¼ˆåŒæ­¥ç‰ˆæœ¬ï¼‰"""
        try:
            results = self.vectorstore.similarity_search_with_score(query, k=k)
            formatted_results = []
            
            for doc, score in results:
                # ChromaDBè¿”å›çš„æ˜¯è·é›¢åˆ†æ•¸ï¼Œè¶Šå°è¶Šç›¸ä¼¼
                distance = score
                similarity = 1.0 / (1.0 + distance)  # è½‰æ›ç‚ºç›¸ä¼¼åº¦åˆ†æ•¸ï¼Œè¶Šå¤§è¶Šç›¸ä¼¼ 1.0 / (1.0 + L2_distance)
                
                result = {
                    "content": doc.page_content,
                    "metadata": doc.metadata,
                    "distance_score": distance,      # è·é›¢åˆ†æ•¸ï¼šè¶Šå°è¶Šç›¸ä¼¼
                    "similarity_score": similarity   # ç›¸ä¼¼åº¦åˆ†æ•¸ï¼šè¶Šå¤§è¶Šç›¸ä¼¼
                }
                formatted_results.append(result)
            
            return formatted_results
        except Exception as e:
            logging.error(f"âŒ æœç´¢å¤±æ•—: {e}")
            return []
    
    async def search_with_vectorstore_async(self, query: str, k: int = 10) -> List[Dict[str, Any]]:
        """ä½¿ç”¨vectorstoreé€²è¡Œç›¸ä¼¼åº¦æœç´¢ï¼ˆç•°æ­¥ç‰ˆæœ¬ï¼‰"""
        async with self.semaphore:  # æ§åˆ¶ä½µç™¼æ•¸é‡
            # åœ¨ç·šç¨‹æ± ä¸­åŸ·è¡ŒåŒæ­¥æ“ä½œ
            loop = asyncio.get_event_loop()
            try:
                formatted_results = await loop.run_in_executor(
                    None, self.search_with_vectorstore, query, k
                )
                return formatted_results
            except Exception as e:
                logging.error(f"âŒ ç•°æ­¥æœç´¢å¤±æ•—: {e}")
                return []
    
    def _rerank_with_api(self, query: str, documents: List[str], top_n: Optional[int] = None) -> List[Dict[str, Any]]:
        """ä½¿ç”¨ Reranker API é€²è¡Œæ‰¹é‡é‡æ’ - å„ªåŒ–ç‰ˆæœ¬ï¼Œæ”¯æŒæ‰¹é‡æ¨ç†å’Œä¸¦ç™¼è™•ç†"""
        try:
            # æ§‹å»ºè«‹æ±‚ - åˆ©ç”¨æ–° API çš„ top_n åƒæ•¸
            request_data = {
                "model": self.reranker_model,
                "query": query,
                "documents": documents
            }
            
            if top_n and top_n > 0:
                request_data["top_n"] = top_n
            
            # æ ¹æ“šæ–‡æª”æ•¸é‡èª¿æ•´è¶…æ™‚æ™‚é–“
            timeout = min(60 + len(documents) * 0.1, 120)
            
            logging.debug(f"ğŸ”„ ç™¼é€ {len(documents)} å€‹æ–‡æª”åˆ° Reranker API (top_n={top_n})")
            
            response = requests.post(
                f"{self.reranker_api_url}/v1/rerank",
                json=request_data,
                timeout=timeout
            )
            
            if response.status_code == 200:
                result = response.json()
                api_results = result["results"]
                usage = result.get("usage", {})
                
                logging.debug(f"âœ… Reranker API æˆåŠŸè™•ç† {usage.get('total_documents', len(documents))} å€‹æ–‡æª”ï¼Œ"
                            f"è¿”å› {usage.get('returned_documents', len(api_results))} å€‹çµæœ")
                
                return api_results
            else:
                logging.error(f"âŒ Reranker API èª¿ç”¨å¤±æ•—: {response.status_code}")
                if response.text:
                    logging.error(f"éŸ¿æ‡‰å…§å®¹: {response.text[:200]}")
                return []
                
        except requests.exceptions.Timeout:
            logging.error(f"âŒ Reranker API èª¿ç”¨è¶…æ™‚ (æ–‡æª”æ•¸: {len(documents)})")
            return []
        except requests.exceptions.ConnectionError:
            logging.error(f"âŒ ç„¡æ³•é€£æ¥åˆ° Reranker API: {self.reranker_api_url}")
            return []
        except Exception as e:
            logging.error(f"âŒ Reranker API èª¿ç”¨ç•°å¸¸: {str(e)}")
            return []
    
    def rerank_search_results(self, query: str, search_results: List[Dict[str, Any]], max_results: Optional[int] = None) -> List[Dict[str, Any]]:
        """å°æœç´¢çµæœé€²è¡Œé‡æ’åº - å„ªåŒ–ç‰ˆæœ¬ï¼Œåˆ©ç”¨æœå‹™å™¨ç«¯æ’åºå’Œæˆªå–"""
        if not self.use_rerank or not search_results:
            return search_results
        
        logging.debug(f"ğŸ”„ å° {len(search_results)} å€‹çµæœé€²è¡Œé‡æ’åº (max_results={max_results})")
        
        # æå–æ–‡æª”å…§å®¹
        documents = [result['content'] for result in search_results]
        
        # åˆ©ç”¨æœå‹™å™¨ç«¯çš„ top_n åƒæ•¸ä¾†æ¸›å°‘ç¶²çµ¡å‚³è¼¸å’Œè™•ç†æ™‚é–“
        # å¦‚æœæŒ‡å®šäº† max_resultsï¼Œè®“æœå‹™å™¨ç«¯åªè¿”å›éœ€è¦çš„æ•¸é‡
        api_top_n = max_results if max_results and max_results < len(documents) else None
        
        # èª¿ç”¨å„ªåŒ–çš„ reranker API
        api_results = self._rerank_with_api(query, documents, top_n=api_top_n)
        
        if not api_results:
            logging.warning("âš ï¸ Reranker API èª¿ç”¨å¤±æ•—ï¼Œä½¿ç”¨åŸå§‹æ’åº")
            # å¦‚æœ API å¤±æ•—ï¼Œä»ç„¶æ‡‰ç”¨ max_results é™åˆ¶
            return search_results[:max_results] if max_results else search_results
        
        # å°‡ API çµæœæ˜ å°„å›åŸå§‹æœç´¢çµæœ
        reranked_results = []
        for api_result in api_results:
            original_idx = api_result["index"]
            if original_idx < len(search_results):
                original_result = search_results[original_idx]
                relevance_score = api_result["relevance_score"]
                
                # æ›´æ–°çµæœï¼Œæ·»åŠ  rerank ä¿¡æ¯
                reranked_result = {
                    **original_result,
                    'rerank_score': relevance_score,
                    'rerank_label': "yes" if relevance_score > 0.5 else "no",
                    'original_rank': original_idx + 1  # è¨˜éŒ„åŸå§‹æ’å
                }
                reranked_results.append(reranked_result)
        
        logging.debug(f"âœ… é‡æ’åºå®Œæˆï¼ŒAPIè¿”å› {len(api_results)} å€‹çµæœï¼Œæ˜ å°„æˆåŠŸ {len(reranked_results)} å€‹")
        
        # API æ‡‰è©²å·²ç¶“æŒ‰ relevance_score æ’åºä¸¦é™åˆ¶äº†æ•¸é‡ï¼Œä½†ç‚ºäº†å®‰å…¨èµ·è¦‹å†æ¬¡ç¢ºä¿
        if max_results and len(reranked_results) > max_results:
            reranked_results = reranked_results[:max_results]
            
        return reranked_results

    def calculate_reciprocal_rank(self, query_item: Dict[str, Any], search_results: List[Dict[str, Any]]) -> Dict[str, Any]:
        """è¨ˆç®—å€’æ•¸æ’å (Reciprocal Rank)ã€R@5 å’Œ R@50 ä¸¦è¿”å›æ’å"""
        expected_doc_id = query_item.get("expected_doc_id")
        
        for rank, result in enumerate(search_results, 1):
            result_id = result.get("metadata", {}).get("id")
            
            # å¦‚æœæ‰¾åˆ°åŒ¹é…çš„é …ç›®
            if result_id == expected_doc_id:
                rr = 1.0 / rank
                r_at_5 = 1.0 if rank <= 5 else 0.0
                r_at_50 = 1.0 if rank <= 50 else 0.0
                
                logging.debug(f"ğŸ¯ æ‰¾åˆ°åŒ¹é…é …ç›® (ID: {expected_doc_id}) åœ¨æ’å {rank}, RR = {rr:.4f}, R@5 = {r_at_5}, R@50 = {r_at_50}")
                
                return {
                    "reciprocal_rank": rr,
                    "rank": rank,
                    "recall_at_5": r_at_5,
                    "recall_at_50": r_at_50
                }
        
        # æ²’æœ‰æ‰¾åˆ°åŒ¹é…é …ç›®ï¼ˆé€™ç¨®æƒ…æ³ä¸æ‡‰è©²ç™¼ç”Ÿï¼‰
        logging.debug(f"âŒ æœªæ‰¾åˆ°åŒ¹é…é …ç›® (ID: {expected_doc_id}), RR = 0")
        return {
            "reciprocal_rank": 0.0,
            "rank": 1027,
            "recall_at_5": 0.0,
            "recall_at_50": 0.0
        }
    
    def get_expected_doc_content(self, doc_id: int) -> str:
        """æ ¹æ“šdoc_idç²å–æœŸæœ›çš„æ–‡æª”å…§å®¹"""
        try:
            # æœç´¢æ•´å€‹è³‡æ–™åº«ä¾†æ‰¾åˆ°åŒ¹é…çš„æ–‡æª”
            all_docs = self.vectorstore.get()
            
            for i, metadata in enumerate(all_docs['metadatas']):
                if metadata.get('id') == doc_id:
                    return all_docs['documents'][i]
            
            # å¦‚æœæ²’æ‰¾åˆ°ï¼Œè¿”å›ç©ºå­—ç¬¦ä¸²
            logging.warning(f"âš ï¸  æœªæ‰¾åˆ°IDç‚º {doc_id} çš„æ–‡æª”")
            return ""
        except Exception as e:
            logging.error(f"âŒ ç²å–æœŸæœ›æ–‡æª”å…§å®¹å¤±æ•—: {e}")
            return ""
    
    async def evaluate_single_query(self, query_item: Dict[str, Any], query_type: str, k: int, query_index: int, save_top_results: int = 3, use_pseudo_doc: bool = False, use_keywords: bool = False) -> Dict[str, Any]:
        """è©•ä¼°å–®å€‹æŸ¥è©¢ï¼ˆç•°æ­¥ç‰ˆæœ¬ï¼‰"""
        total_start_time = time.time()
        
        query_text = self.generate_query_text(query_item, query_type, use_pseudo_doc, use_keywords)
        
        if not query_text:
            logging.warning(f"âš ï¸  æŸ¥è©¢ {query_index} çš„æ–‡æœ¬ç‚ºç©ºï¼Œè·³é")
            return None
        
        # éšæ®µ1: åŸ·è¡Œembeddingæª¢ç´¢
        embedding_start_time = time.time()
        search_k = self.rerank_top_n if self.use_rerank else k
        embedding_results = await self.search_with_vectorstore_async(query_text, search_k)
        embedding_time = time.time() - embedding_start_time
        
        # åˆå§‹åŒ–æ™‚é–“çµ±è¨ˆ
        rerank_time = 0.0
        
        if not embedding_results:
            logging.warning(f"âš ï¸  æŸ¥è©¢ {query_index} æ²’æœ‰è¿”å›çµæœ")
            metrics = {
                "reciprocal_rank": 0.0,
                "rank": 1027,
                "recall_at_5": 0.0,
                "recall_at_50": 0.0
            }
            search_results = []
        else:
            # å¦‚æœå•Ÿç”¨rerankï¼Œé€²è¡Œå…©éšæ®µæª¢ç´¢
            if self.use_rerank:
                # éšæ®µ2: ä½¿ç”¨rerankeré‡æ’åºï¼Œåˆ©ç”¨æœå‹™å™¨ç«¯å„ªåŒ–
                rerank_start_time = time.time()
                # ç›´æ¥æŒ‡å®šæœ€çµ‚éœ€è¦çš„æ•¸é‡ï¼Œè®“æœå‹™å™¨ç«¯è™•ç†æ’åºå’Œæˆªå–
                search_results = self.rerank_search_results(query_text, embedding_results, max_results=k)
                rerank_time = time.time() - rerank_start_time
                
                logging.debug(f"ğŸ¯ æŸ¥è©¢ {query_index} å…©éšæ®µæª¢ç´¢: {len(embedding_results)} -> {len(search_results)} (æœå‹™å™¨ç«¯å„ªåŒ–)")
            else:
                search_results = embedding_results
            
            metrics = self.calculate_reciprocal_rank(query_item, search_results)
        
        total_time = time.time() - total_start_time
        
        # ç²å–æœŸæœ›æ–‡æª”å…§å®¹
        expected_doc_content = self.get_expected_doc_content(query_item.get("expected_doc_id"))
        
        result = {
            "user_id": query_item.get("user_id"),
            "expected_answer": query_item.get("expected_answer"),
            "expected_doc_id": query_item.get("expected_doc_id"),
            "query_text": query_text,
            "reciprocal_rank": metrics["reciprocal_rank"],
            "expected_doc_rank": metrics["rank"],
            "recall_at_5": metrics["recall_at_5"],
            "recall_at_50": metrics["recall_at_50"],
            "search_results": search_results[:save_top_results], # ä¿ç•™å‰Nå€‹çµæœ
            "expected_doc_content": expected_doc_content,
            "timing": {
                "total_time": total_time,
                "embedding_time": embedding_time,
                "rerank_time": rerank_time,
                "use_rerank": self.use_rerank
            }
        }
        
        return result
    
    async def evaluate_queries_async(self, queries: List[Dict[str, Any]], query_type: str = "prompt", k: int = 1027, save_top_results: int = 3, use_pseudo_doc: bool = False, use_keywords: bool = False) -> Dict[str, Any]:
        """è©•ä¼°æŸ¥è©¢åˆ—è¡¨ï¼ˆç•°æ­¥ç‰ˆæœ¬ï¼‰"""
        logging.info(f"ğŸ” é–‹å§‹ç•°æ­¥è©•ä¼° {len(queries)} å€‹æŸ¥è©¢...")
        logging.info(f"âš¡ ä½¿ç”¨ {self.max_concurrent} å€‹ä½µç™¼é€£æ¥")
        if use_keywords:
            logging.info(f"ğŸ”‘ å·²å•Ÿç”¨é—œéµè©æª¢ç´¢æ–¹æ³•ï¼šä½¿ç”¨keywordså­—æ®µé€²è¡Œæª¢ç´¢")
        elif use_pseudo_doc:
            logging.info(f"ğŸ“„ å·²å•Ÿç”¨Query2Docæ–¹æ³•ï¼šå°‡åŸå§‹queryèˆ‡pseudo_documentçµåˆæª¢ç´¢")
        else:
            logging.info(f"ğŸ“ ä½¿ç”¨åŸå§‹queryé€²è¡Œæª¢ç´¢")
        
        if self.use_rerank:
            logging.info(f"ğŸ”„ ä½¿ç”¨å…©éšæ®µæª¢ç´¢ï¼šEmbedding Top-{self.rerank_top_n} -> Reranker (æ‰¹é‡å„ªåŒ–)")
            if len(queries) >= 10:
                logging.info(f"ğŸš€ å¤§æ‰¹é‡è™•ç†æ¨¡å¼ï¼šAPIå°‡ä½¿ç”¨æ‰¹é‡æ¨ç†åŠ é€Ÿè™•ç†")
        
        # å‰µå»ºæ‰€æœ‰æŸ¥è©¢ä»»å‹™
        tasks = []
        for i, query_item in enumerate(queries):
            task = self.evaluate_single_query(query_item, query_type, k, i + 1, save_top_results, use_pseudo_doc, use_keywords)
            tasks.append(task)
        
        # ä½¿ç”¨ tqdm é¡¯ç¤ºé€²åº¦æ¢ä¸¦ç­‰å¾…æ‰€æœ‰ä»»å‹™å®Œæˆ
        results_raw = []
        with sync_tqdm(total=len(tasks), desc="ğŸ” Processing queries", unit="query") as pbar:
            completed_tasks = asyncio.as_completed(tasks)
            for completed_task in completed_tasks:
                result_data = await completed_task
                if result_data is not None:
                    results_raw.append(result_data)
                pbar.update(1)
        
        # æŒ‰åŸå§‹é †åºæ’åºçµæœ
        results = []
        reciprocal_ranks = []
        recall_at_5_scores = []
        recall_at_50_scores = []
        
        for i, query_item in enumerate(queries):
            # æ‰¾åˆ°å°æ‡‰çš„çµæœ
            matching_result = None
            for result in results_raw:
                if result["user_id"] == query_item.get("user_id") and result["expected_doc_id"] == query_item.get("expected_doc_id"):
                    matching_result = result
                    break
            
            if matching_result:
                results.append(matching_result)
                reciprocal_ranks.append(matching_result["reciprocal_rank"])
                recall_at_5_scores.append(matching_result["recall_at_5"])
                recall_at_50_scores.append(matching_result["recall_at_50"])
        
        # è¨ˆç®—å¹³å‡æŒ‡æ¨™
        mrr = sum(reciprocal_ranks) / len(reciprocal_ranks) if reciprocal_ranks else 0.0
        mean_recall_at_5 = sum(recall_at_5_scores) / len(recall_at_5_scores) if recall_at_5_scores else 0.0
        mean_recall_at_50 = sum(recall_at_50_scores) / len(recall_at_50_scores) if recall_at_50_scores else 0.0
        
        # è¨ˆç®—æ™‚é–“çµ±è¨ˆ
        total_times = [result.get('timing', {}).get('total_time', 0) for result in results]
        embedding_times = [result.get('timing', {}).get('embedding_time', 0) for result in results]
        rerank_times = [result.get('timing', {}).get('rerank_time', 0) for result in results]
        
        timing_stats = {
            "avg_total_time": sum(total_times) / len(total_times) if total_times else 0.0,
            "avg_embedding_time": sum(embedding_times) / len(embedding_times) if embedding_times else 0.0,
            "avg_rerank_time": sum(rerank_times) / len(rerank_times) if rerank_times else 0.0,
            "total_queries_time": sum(total_times),
            "total_embedding_time": sum(embedding_times),
            "total_rerank_time": sum(rerank_times)
        }
        
        evaluation_summary = {
            "database": self.db_name,
            "embedding_model": self.embedding_model,
            "query_type": query_type,
            "use_pseudo_doc": use_pseudo_doc,
            "use_keywords": use_keywords,
            "use_rerank": self.use_rerank,
            "reranker_model": self.reranker_model if self.use_rerank else None,
            "rerank_top_n": self.rerank_top_n if self.use_rerank else None,
            "num_queries": len(queries),
            "num_evaluated": len(results),
            "mean_reciprocal_rank": mrr,
            "mean_recall_at_5": mean_recall_at_5,
            "mean_recall_at_50": mean_recall_at_50,
            "timing_stats": timing_stats,
            "individual_results": results
        }
        
        logging.info(f"ğŸ“Š ç•°æ­¥è©•ä¼°å®Œæˆ!")
        logging.info(f"ğŸ“ˆ å¹³å‡å€’æ•¸æ’å (MRR): {mrr:.4f}")
        logging.info(f"ğŸ“ˆ å¹³å‡ Recall@5: {mean_recall_at_5:.4f}")
        logging.info(f"ğŸ“ˆ å¹³å‡ Recall@50: {mean_recall_at_50:.4f}")
        logging.info(f"â±ï¸  å¹³å‡æŸ¥è©¢æ™‚é–“: {timing_stats['avg_total_time']:.3f}ç§’")
        logging.info(f"ğŸ” å¹³å‡embeddingæ™‚é–“: {timing_stats['avg_embedding_time']:.3f}ç§’")
        if self.use_rerank:
            logging.info(f"ğŸ”„ å¹³å‡rerankæ™‚é–“: {timing_stats['avg_rerank_time']:.3f}ç§’")
        
        return evaluation_summary
    
    def evaluate_queries(self, queries: List[Dict[str, Any]], query_type: str = "prompt", k: int = 1027, save_top_results: int = 3, use_pseudo_doc: bool = False, use_keywords: bool = False) -> Dict[str, Any]:
        """è©•ä¼°æŸ¥è©¢åˆ—è¡¨ï¼ˆåŒæ­¥å…¥å£ï¼‰"""
        # é‹è¡Œç•°æ­¥ç‰ˆæœ¬
        return asyncio.run(self.evaluate_queries_async(queries, query_type, k, save_top_results, use_pseudo_doc, use_keywords))
    
    def save_results(self, results: Dict[str, Any], output_folder: str = None, use_pseudo_doc: bool = False, use_keywords: bool = False) -> str:
        """ä¿å­˜è©•ä¼°çµæœåˆ°æ–‡ä»¶å¤¾"""
        # å‰µå»ºåŸºç¤outputsç›®éŒ„
        os.makedirs("outputs", exist_ok=True)
        
        if output_folder is None:
            # ä½¿ç”¨é»˜èªæ–‡ä»¶å¤¾å‘½å
            folder_parts = [f"run_{self.db_name}_{results['num_queries']}"]
            
            if use_keywords:
                folder_parts.append("with_keywords")
            elif use_pseudo_doc:
                folder_parts.append("with_pseudo")
            
            if self.use_rerank:
                # å°‡æ¨¡å‹åç¨±è½‰æ›ç‚ºé©åˆæ–‡ä»¶å¤¾åç¨±çš„æ ¼å¼ï¼ˆæ›¿æ›ç‰¹æ®Šå­—ç¬¦ï¼‰
                safe_model_name = self.reranker_model.replace("/", "_").replace(":", "_")
                folder_parts.append(f"with_{safe_model_name}")
            
            folder_name = "_".join(folder_parts)
            output_folder = f"outputs/{folder_name}"
        else:
            # å¦‚æœç”¨æˆ¶æŒ‡å®šäº†æ–‡ä»¶å¤¾ä½†æ²’æœ‰åŒ…å«outputsè·¯å¾‘ï¼Œè‡ªå‹•æ·»åŠ 
            if not output_folder.startswith("outputs/"):
                output_folder = f"outputs/{output_folder}"
        
        # å‰µå»ºè¼¸å‡ºæ–‡ä»¶å¤¾
        os.makedirs(output_folder, exist_ok=True)
        
        # ä¿å­˜ä¸»è¦è©•ä¼°çµæœ
        result_file = os.path.join(output_folder, "evaluation_results.json")
        
        try:
            with open(result_file, 'w', encoding='utf-8') as f:
                json.dump(results, f, ensure_ascii=False, indent=2)
            
            logging.info(f"ğŸ’¾ çµæœå·²ä¿å­˜åˆ°æ–‡ä»¶å¤¾: {output_folder}")
            logging.info(f"ğŸ“‹ è©•ä¼°çµæœæ–‡ä»¶: {result_file}")
            
            return output_folder  # è¿”å›æ–‡ä»¶å¤¾è·¯å¾‘ä¾›å…¶ä»–æ–¹æ³•ä½¿ç”¨
        except Exception as e:
            logging.error(f"âŒ ä¿å­˜çµæœå¤±æ•—: {e}")
            return None
    
    def print_summary(self, results: Dict[str, Any]):
        """æ‰“å°è©•ä¼°æ‘˜è¦"""
        print("\n" + "="*70)
        print("ğŸ“Š æª¢ç´¢è©•ä¼°çµæœæ‘˜è¦")
        print("="*70)
        print(f"æ•¸æ“šåº«: {results['database']}")
        print(f"Embeddingæ¨¡å‹: {results['embedding_model']}")
        print(f"æŸ¥è©¢é¡å‹: {results['query_type']}")
        print(f"é—œéµè©æª¢ç´¢: {'å·²å•Ÿç”¨' if results.get('use_keywords', False) else 'æœªå•Ÿç”¨'}")
        print(f"Query2Doc: {'å·²å•Ÿç”¨' if results.get('use_pseudo_doc', False) else 'æœªå•Ÿç”¨'}")
        print(f"å…©éšæ®µæª¢ç´¢: {'å·²å•Ÿç”¨' if results.get('use_rerank', False) else 'æœªå•Ÿç”¨'}")
        if results.get('use_rerank', False):
            print(f"Rerankeræ¨¡å‹: {results.get('reranker_model', 'N/A')}")
            print(f"Rerank Top-N: {results.get('rerank_top_n', 'N/A')}")
        print(f"æŸ¥è©¢æ•¸é‡: {results['num_queries']}")
        print(f"è©•ä¼°æ•¸é‡: {results['num_evaluated']}")
        print(f"å¹³å‡å€’æ•¸æ’å (MRR): {results['mean_reciprocal_rank']:.4f}")
        print(f"å¹³å‡ Recall@5: {results['mean_recall_at_5']:.4f}")
        print(f"å¹³å‡ Recall@50: {results['mean_recall_at_50']:.4f}")
        
        # é¡¯ç¤ºæ™‚é–“çµ±è¨ˆ
        timing_stats = results.get('timing_stats', {})
        if timing_stats:
            print(f"\nâ±ï¸  æ™‚é–“çµ±è¨ˆ:")
            print(f"å¹³å‡æŸ¥è©¢æ™‚é–“: {timing_stats.get('avg_total_time', 0):.3f}ç§’")
            print(f"å¹³å‡embeddingæ™‚é–“: {timing_stats.get('avg_embedding_time', 0):.3f}ç§’")
            if results.get('use_rerank', False):
                print(f"å¹³å‡rerankæ™‚é–“: {timing_stats.get('avg_rerank_time', 0):.3f}ç§’")
                rerank_ratio = (timing_stats.get('avg_rerank_time', 0) / timing_stats.get('avg_total_time', 1)) * 100
                print(f"rerankä½”æ¯”: {rerank_ratio:.1f}%")
            print(f"ç¸½è™•ç†æ™‚é–“: {timing_stats.get('total_queries_time', 0):.3f}ç§’")
        
        # é¡¯ç¤ºå€‹åˆ¥çµæœ
        print("\nğŸ“‹ å€‹åˆ¥æŸ¥è©¢çµæœ:")
        if results.get('use_rerank', False):
            print(f"{'No.':<4} {'Expected Answer':<20} {'RR':<8} {'R@5':<6} {'R@50':<6} {'ç¸½æ™‚é–“':<8} {'Emb':<6} {'Rerank':<6}")
            print("-" * 84)
        else:
            print(f"{'No.':<4} {'Expected Answer':<25} {'RR':<8} {'R@5':<6} {'R@50':<6} {'æ™‚é–“':<8}")
            print("-" * 70)
        
        for i, result in enumerate(results['individual_results'], 1):
            expected_answer = result.get('expected_answer', 'N/A')
            rr = result['reciprocal_rank']
            r5 = result['recall_at_5']
            r50 = result['recall_at_50']
            timing = result.get('timing', {})
            total_time = timing.get('total_time', 0)
            emb_time = timing.get('embedding_time', 0)
            rerank_time = timing.get('rerank_time', 0)
            
            if results.get('use_rerank', False):
                print(f"{i:<4} {expected_answer[:19]:<20} {rr:<8.4f} {r5:<6.1f} {r50:<6.1f} {total_time:<8.3f} {emb_time:<6.3f} {rerank_time:<6.3f}")
            else:
                print(f"{i:<4} {expected_answer[:24]:<25} {rr:<8.4f} {r5:<6.1f} {r50:<6.1f} {total_time:<8.3f}")
        
        print("="*70)
    
    def visualize_ranking_distribution(self, results: Dict[str, Any], output_folder: str):
        """ç”Ÿæˆexpected_doc_idæ’ååˆ†ä½ˆå¯è¦–åŒ–åœ–è¡¨ï¼ˆåˆ†å¸ƒåœ–å’Œç´¯ç©åœ–åˆ†é–‹ä¿å­˜ï¼‰ï¼ŒåŒ…å«l2_distanceå¹³å‡å€¼æŠ˜ç·šåœ–"""
        try:
            # ç¢ºä¿è¼¸å‡ºæ–‡ä»¶å¤¾å­˜åœ¨
            os.makedirs(output_folder, exist_ok=True)
            
            # è¨­å®šåœ–è¡¨è¼¸å‡ºè·¯å¾‘
            hist_output_file = os.path.join(output_folder, "ranking_histogram.png")
            cumulative_output_file = os.path.join(output_folder, "ranking_cumulative.png")
            
            # å¾å·²ä¿å­˜çš„çµæœä¸­ç²å–æ’åå’Œè·é›¢ä¿¡æ¯
            rankings = []
            rank_distances = {}  # ç”¨æ–¼æ”¶é›†æ¯å€‹æ’åä½ç½®çš„æ‰€æœ‰è·é›¢å€¼
            total_queries = len(results['individual_results'])
            
            print(f"ğŸ” Extracting ranking positions and distances from {total_queries} queries...")
            
            for i, result in enumerate(results['individual_results'], 1):
                expected_doc_id = result['expected_doc_id']
                expected_doc_rank = result.get('expected_doc_rank', 1027)
                search_results = result.get('search_results', [])
                
                rankings.append(expected_doc_rank)
                print(f"   Query {i}: Expected doc_id {expected_doc_id} found at rank {expected_doc_rank}")
                
                # æ”¶é›†æ¯å€‹æ’åä½ç½®çš„l2_distance
                for rank_idx, search_result in enumerate(search_results):
                    rank_pos = rank_idx + 1  # æ’åå¾1é–‹å§‹
                    distance = search_result.get('distance_score', 0)
                    
                    if rank_pos not in rank_distances:
                        rank_distances[rank_pos] = []
                    rank_distances[rank_pos].append(distance)
            
            # è¨ˆç®—æ¯å€‹æ’åä½ç½®çš„å¹³å‡l2_distanceå’Œè®ŠåŒ–ç¯„åœ
            avg_distances_by_rank = {}
            std_distances_by_rank = {}
            min_distances_by_rank = {}
            max_distances_by_rank = {}
            max_rank_with_data = 0
            
            for rank, distances in rank_distances.items():
                if distances:  # ç¢ºä¿æœ‰æ•¸æ“š
                    avg_distances_by_rank[rank] = np.mean(distances)
                    std_distances_by_rank[rank] = np.std(distances) if len(distances) > 1 else 0.0
                    min_distances_by_rank[rank] = np.min(distances)
                    max_distances_by_rank[rank] = np.max(distances)
                    max_rank_with_data = max(max_rank_with_data, rank)
            
            print(f"ğŸ“Š Calculated average l2_distance for ranks 1-{max_rank_with_data}")
            
            # ç¬¬ä¸€å¼µåœ–ï¼šåˆ†å¸ƒç›´æ–¹åœ– + l2_distanceæŠ˜ç·šåœ–
            fig, ax1 = plt.subplots(figsize=(12, 8))
            
            # ä¸»è»¸ï¼šåˆ†å¸ƒç›´æ–¹åœ–
            bins = range(1, 1029)  # å¾1åˆ°1028ï¼ŒåŒ…å«1027å€‹æ’åä½ç½®
            n, bins_edges, patches = ax1.hist(rankings, bins=bins, alpha=0.7, color='skyblue', edgecolor='black', linewidth=0.5)
            ax1.set_xlabel('Rank of Expected Doc ID', fontsize=12)
            ax1.set_ylabel('Count', fontsize=12, color='blue')
            ax1.tick_params(axis='y', labelcolor='blue')
            ax1.set_xlim(1, min(1027, max_rank_with_data + 50))
            ax1.grid(True, alpha=0.3)
            
            # æ·»åŠ çµ±è¨ˆä¿¡æ¯åˆ°åˆ†å¸ƒåœ–
            if rankings:
                mean_rank = np.mean(rankings)
                median_rank = np.median(rankings)
                ax1.axvline(mean_rank, color='red', linestyle='--', alpha=0.7, label=f'Mean Rank: {mean_rank:.1f}')
                ax1.axvline(median_rank, color='orange', linestyle='--', alpha=0.7, label=f'Median Rank: {median_rank:.1f}')
            
            # å‰¯è»¸ï¼šl2_distanceæŠ˜ç·šåœ–
            ax2 = ax1.twinx()
            if avg_distances_by_rank:
                ranks_for_line = sorted(avg_distances_by_rank.keys())
                distances_for_line = [avg_distances_by_rank[rank] for rank in ranks_for_line]
                
                # è¨ˆç®—ä¸Šä¸‹ç•Œç¯„åœï¼ˆä½¿ç”¨æ¨™æº–å·®ï¼‰
                std_upper = [avg_distances_by_rank[rank] + std_distances_by_rank[rank] for rank in ranks_for_line]
                std_lower = [avg_distances_by_rank[rank] - std_distances_by_rank[rank] for rank in ranks_for_line]
                
                # ç¹ªè£½ç¯„åœå¸¶ï¼ˆæ·ºç¶ è‰²ï¼‰
                ax2.fill_between(ranks_for_line, std_lower, std_upper, 
                                color='lightgreen', alpha=0.3, label='Â±1 Std Dev Range')
                
                # ç¹ªè£½å¹³å‡å€¼æŠ˜ç·šåœ–ï¼ˆç¶ è‰²ï¼‰
                ax2.plot(ranks_for_line, distances_for_line, color='darkgreen', linewidth=2, marker='o', 
                        markersize=3, alpha=0.8, label='Average L2 Distance')
                ax2.set_ylabel('Average L2 Distance', fontsize=12, color='darkgreen')
                ax2.tick_params(axis='y', labelcolor='darkgreen')
                
                # æ·»åŠ ç™¾åˆ†ä½æ•¸æ¨™è¨˜ï¼ˆPR25, 50, 75ï¼‰
                percentiles = [25, 50, 75]
                percentile_colors = ['purple', 'magenta', 'skyblue']
                percentile_values = np.percentile(distances_for_line, percentiles)
                
                for i, (pct, value, color) in enumerate(zip(percentiles, percentile_values, percentile_colors)):
                    ax2.axhline(value, color=color, linestyle='-.', alpha=0.8, linewidth=1.5,
                               label=f'PR{pct}: {value:.3f}')

            # çµ±ä¸€æ¨™é¡Œå’Œåœ–ä¾‹
            plt.title(f'Ranking Distribution with Average L2 Distance\n(Total Queries: {total_queries})', fontsize=14)
            
            # åˆä½µåœ–ä¾‹
            lines1, labels1 = ax1.get_legend_handles_labels()
            lines2, labels2 = ax2.get_legend_handles_labels()
            ax1.legend(lines1 + lines2, labels1 + labels2, loc='upper right')
            
            plt.tight_layout()
            plt.savefig(hist_output_file, dpi=300, bbox_inches='tight')
            plt.close()
            
            # ç¬¬äºŒå¼µåœ–ï¼šç´¯ç©åˆ†å¸ƒåœ–
            plt.figure(figsize=(10, 6))
            sorted_rankings = sorted(rankings)
            # è¨ˆç®—ç´¯ç©æ¯”ä¾‹
            cumulative_counts = np.arange(1, len(sorted_rankings) + 1) / len(sorted_rankings)
            
            plt.plot(sorted_rankings, cumulative_counts, marker='o', markersize=4, 
                    linewidth=2, color='darkgreen', alpha=0.8)
            plt.xlabel('Rank of Expected Doc ID')
            plt.ylabel('Cumulative Proportion')
            plt.title(f'Cumulative Distribution of Rankings\n(Total Queries: {total_queries})')
            plt.grid(True, alpha=0.3)
            plt.xlim(1, 1027)
            plt.ylim(0, 1)
            
            # æ·»åŠ é‡è¦çš„ç´¯ç©æŒ‡æ¨™ç·šï¼Œæ¯å€‹ä½¿ç”¨ä¸åŒé¡è‰²
            recall_points = [5, 10, 20, 50, 100]
            colors = ['red', 'blue', 'purple', 'orange', 'brown']
            
            for i, recall_k in enumerate(recall_points):
                recall_proportion = sum(1 for rank in rankings if rank <= recall_k) / len(rankings)
                if recall_proportion > 0:
                    color = colors[i % len(colors)]
                    plt.axhline(recall_proportion, color=color, linestyle=':', alpha=0.7, 
                               label=f'Recall@{recall_k}: {recall_proportion:.4f}')
                    plt.axvline(recall_k, color=color, linestyle=':', alpha=0.7)
            
            plt.legend(fontsize=9)
            plt.tight_layout()
            plt.savefig(cumulative_output_file, dpi=300, bbox_inches='tight')
            plt.close()
            
            print(f"ğŸ“Š Histogram chart saved to: {hist_output_file}")
            print(f"ğŸ“Š Cumulative distribution chart saved to: {cumulative_output_file}")
            
            # æ‰“å°è©³ç´°çµ±è¨ˆä¿¡æ¯
            print(f"ğŸ“ˆ Ranking Statistics:")
            print(f"   Total queries analyzed: {total_queries}")
            print(f"   Mean rank: {np.mean(rankings):.1f}")
            print(f"   Median rank: {np.median(rankings):.1f}")
            print(f"   Best rank: {min(rankings)}")
            print(f"   Worst rank: {max(rankings)}")
            
            # æ‰“å°L2è·é›¢çµ±è¨ˆä¿¡æ¯
            if avg_distances_by_rank:
                all_avg_distances = list(avg_distances_by_rank.values())
                all_std_distances = list(std_distances_by_rank.values())
                print(f"ğŸ“ L2 Distance Statistics:")
                print(f"   Ranks with distance data: 1-{max_rank_with_data}")
                print(f"   Mean average distance: {np.mean(all_avg_distances):.4f}")
                print(f"   Median average distance: {np.median(all_avg_distances):.4f}")
                print(f"   Mean standard deviation: {np.mean(all_std_distances):.4f}")
                print(f"   Min average distance: {min(all_avg_distances):.4f} (rank {min(avg_distances_by_rank, key=avg_distances_by_rank.get)})")
                print(f"   Max average distance: {max(all_avg_distances):.4f} (rank {max(avg_distances_by_rank, key=avg_distances_by_rank.get)})")
                
                # é¡¯ç¤ºè®ŠåŒ–ç¯„åœæœ€å¤§çš„å¹¾å€‹rank
                rank_variance = [(rank, std) for rank, std in std_distances_by_rank.items() if std > 0]
                if rank_variance:
                    rank_variance.sort(key=lambda x: x[1], reverse=True)
                    print(f"   Top 3 ranks with highest variance:")
                    for i, (rank, std) in enumerate(rank_variance[:3]):
                        print(f"     Rank {rank}: std={std:.4f}, avg={avg_distances_by_rank[rank]:.4f}")
            
            # æ·»åŠ ç´¯ç©çµ±è¨ˆä¿¡æ¯
            print(f"ğŸ“Š Cumulative Statistics:")
            for recall_k in [5, 10, 20, 50, 100]:
                count_within_k = sum(1 for rank in rankings if rank <= recall_k)
                proportion = count_within_k / total_queries
                # åŒæ™‚é¡¯ç¤ºè©²rankè™•çš„å¹³å‡l2è·é›¢å’Œæ¨™æº–å·®
                avg_distance_at_k = avg_distances_by_rank.get(recall_k, "N/A")
                std_distance_at_k = std_distances_by_rank.get(recall_k, "N/A")
                if isinstance(avg_distance_at_k, float) and isinstance(std_distance_at_k, float):
                    print(f"   Recall@{recall_k}: {count_within_k}/{total_queries} = {proportion:.4f}, Avg L2 distance: {avg_distance_at_k:.4f}Â±{std_distance_at_k:.4f}")
                else:
                    print(f"   Recall@{recall_k}: {count_within_k}/{total_queries} = {proportion:.4f}, Avg L2 distance: {avg_distance_at_k}")
            
            print(f"   Rank distribution: {sorted(rankings)}")
                
        except Exception as e:
            logging.error(f"âŒ å¯è¦–åŒ–ç”Ÿæˆå¤±æ•—: {e}")
            import traceback
            traceback.print_exc()
    
    def generate_simple_recall_curve(self, results: Dict[str, Any], output_folder: str):
        """ç”Ÿæˆç²¾ç°¡çš„recallæ›²ç·šåœ–ï¼ˆåªé¡¯ç¤ºrecall@5,10,20,50,100äº”å€‹é—œéµé»ï¼‰"""
        try:
            # ç¢ºä¿è¼¸å‡ºæ–‡ä»¶å¤¾å­˜åœ¨
            os.makedirs(output_folder, exist_ok=True)
            
            # è¨­å®šåœ–è¡¨è¼¸å‡ºè·¯å¾‘
            recall_curve_output_file = os.path.join(output_folder, "recall_curve_simplified.png")
            
            # å¾çµæœä¸­ç²å–æ’åä¿¡æ¯
            rankings = []
            for result in results['individual_results']:
                expected_doc_rank = result.get('expected_doc_rank', 1027)
                rankings.append(expected_doc_rank)
            
            total_queries = len(rankings)
            
            # è¨ˆç®—é—œéµrecallé»
            recall_k_values = [5, 10, 20, 50, 100]
            recall_proportions = []
            
            for recall_k in recall_k_values:
                count_within_k = sum(1 for rank in rankings if rank <= recall_k)
                proportion = count_within_k / total_queries
                recall_proportions.append(proportion)
            
            # ç”Ÿæˆç²¾ç°¡çš„recallæ›²ç·šåœ–
            plt.figure(figsize=(10, 6))
            plt.plot(recall_k_values, recall_proportions, 
                    marker='o', markersize=6, linewidth=3, 
                    color='darkblue', alpha=0.8, markerfacecolor='darkblue', 
                    markeredgecolor='darkblue', markeredgewidth=2)
            
            plt.xlabel('K (Rank Threshold)', fontsize=12)
            plt.ylabel('Recall@K', fontsize=12)
            plt.title(f'Simplified Recall Curve\n(Total Queries: {total_queries})', fontsize=14)
            plt.grid(True, alpha=0.3)
            plt.xlim(0, 1027)
            plt.ylim(0, max(1.0, max(recall_proportions) + 0.1))
            
            plt.tight_layout()
            plt.savefig(recall_curve_output_file, dpi=300, bbox_inches='tight')
            plt.close()
            
            print(f"ğŸ“Š Simplified recall curve saved to: {recall_curve_output_file}")
            
            # æ‰“å°recallçµ±è¨ˆä¿¡æ¯
            print(f"ğŸ“ˆ Recall Statistics:")
            for k, recall in zip(recall_k_values, recall_proportions):
                count = int(recall * total_queries)
                print(f"   Recall@{k}: {count}/{total_queries} = {recall:.3f}")
                
        except Exception as e:
            logging.error(f"âŒ ç²¾ç°¡recallæ›²ç·šç”Ÿæˆå¤±æ•—: {e}")
            import traceback
            traceback.print_exc()


def main():
    parser = argparse.ArgumentParser(description="æª¢ç´¢è©•ä¼°å·¥å…·")
    parser.add_argument("--json-file", default="data/tcm_sd_test_rc_direct.json",
                       help="æ¸¬è©¦æ•¸æ“šJSONæ–‡ä»¶è·¯å¾‘ (é»˜èª: data/tcm_sd_test_rc_direct.json)")
    parser.add_argument("--db-name", default="syndrome_db",
                       help="ChromaDBæ•¸æ“šåº«åç¨± (é»˜èª: syndrome_db)")
    parser.add_argument("--num-queries", type=int, default=10,
                       help="éš¨æ©Ÿé¸æ“‡çš„æŸ¥è©¢æ•¸é‡ (é»˜èª: 10)")
    parser.add_argument("--query-type", choices=["prompt"],
                       default="prompt", help="æŸ¥è©¢æ–‡æœ¬é¡å‹ (é»˜èª: prompt)")
    parser.add_argument("--k", type=int, default=1027,
                       help="æª¢ç´¢çµæœæ•¸é‡ (é»˜èª: 1027)")
    parser.add_argument("--output-folder",
                       help="è¼¸å‡ºæ–‡ä»¶å¤¾è·¯å¾‘ (å¯é¸, é»˜èªç‚º outputs/run_{db_name}_{num_queries})")
    parser.add_argument("--seed", type=int, default=42,
                       help="éš¨æ©Ÿç¨®å­ (å¯é‡ç¾çµæœ)")
    parser.add_argument("--embedding-model", default="text-embedding-3-large",
                       help="Embeddingæ¨¡å‹ (é»˜èª: text-embedding-3-large)ã€‚æ ¼å¼: 'provider:model' æˆ– 'model'ã€‚æ”¯æŒ openai, vllm, custom, huggingface")
    parser.add_argument("--max-concurrent", type=int, default=10,
                       help="æœ€å¤§ä½µç™¼è«‹æ±‚æ•¸ (é»˜èª: 10)")
    parser.add_argument("--save-top-results", type=int, default=1027,
                       help="ä¿å­˜åˆ°çµæœä¸­çš„æœç´¢çµæœæ•¸é‡ (é»˜èª: 1027)")
    parser.add_argument("--use-pseudo-doc", action="store_true", default=False,
                       help="å•Ÿç”¨Query2Docæ–¹æ³•ï¼šå°‡åŸå§‹queryèˆ‡pseudo_documentçµåˆé€²è¡Œæª¢ç´¢ (é»˜èª: False)")
    parser.add_argument("--use-keywords", action="store_true", default=False,
                       help="å•Ÿç”¨é—œéµè©æª¢ç´¢æ–¹æ³•ï¼šä½¿ç”¨keywordså­—æ®µé€²è¡Œæª¢ç´¢ (é»˜èª: False)")
    parser.add_argument("--list-models", action="store_true",
                       help="åˆ—å‡ºæ‰€æœ‰å¯ç”¨çš„embeddingæ¨¡å‹ä¸¦é€€å‡º")
    parser.add_argument("--rerank", action="store_true", default=False,
                       help="å•Ÿç”¨å…©éšæ®µæª¢ç´¢ï¼šå…ˆç”¨embeddingæ’åºå–top-kï¼Œå†ç”¨rerankeré‡æ’ (é»˜èª: False)")
    parser.add_argument("--reranker-api-url", default="http://localhost:8001",
                       help="Reranker API URL (é»˜èª: http://localhost:8001)")
    parser.add_argument("--reranker-model", default="Qwen/Qwen3-Reranker-0.6B",
                       help="Rerankeræ¨¡å‹åç¨± (é»˜èª: Qwen/Qwen3-Reranker-0.6B)")
    parser.add_argument("--rerank-top-n", type=int, default=100,
                       help="é€çµ¦rerankerçš„å€™é¸æ–‡æª”æ•¸é‡ (é»˜èª: 100)")
    
    args = parser.parse_args()
    
    # Handle --list-models option
    if args.list_models:
        print("Available embedding models by provider:")
        print("=" * 50)
        
        for provider, config in EMBEDDING_PROVIDERS.items():
            print(f"\n{provider.upper()}:")
            for model in config["models"]:
                default_marker = " (default)" if model == config["default"] else ""
                print(f"  - {provider}:{model}{default_marker}")
        
        print(f"\nUsage examples:")
        print(f"  --embedding-model openai:text-embedding-3-large")
        print(f"  --embedding-model vllm:Qwen3-Embedding-8B")
        print(f"  --embedding-model custom:Qwen3-Embedding-0.6B-finetuned")
        print(f"  --embedding-model huggingface:BAAI/bge-large-zh-v1.5")
        print(f"  --embedding-model text-embedding-3-large  # OpenAI provider")
        return
    
    # æª¢æŸ¥embeddingæ¨¡å‹çš„è¦æ±‚
    try:
        provider, model = get_provider_and_model(args.embedding_model)
        
        # åªæœ‰ç•¶ä½¿ç”¨OpenAIæä¾›è€…æ™‚æ‰æª¢æŸ¥API Key
        if provider == "openai" and not os.getenv("OPENAI_API_KEY"):
            print("âŒ OPENAI_API_KEY ç’°å¢ƒè®Šé‡æœªè¨­ç½®")
            print("è«‹è¨­ç½® API key å¾Œå†é‹è¡Œ")
            sys.exit(1)
            
        logging.info(f"ä½¿ç”¨ {provider} embedding æ¨¡å‹: {model}")
        
    except ValueError as e:
        logging.error(str(e))
        print("ä½¿ç”¨ --list-models æŸ¥çœ‹å¯ç”¨çš„embeddingæ¨¡å‹")
        sys.exit(1)
    
    # è¨­ç½®éš¨æ©Ÿç¨®å­
    if args.seed:
        random.seed(args.seed)
        logging.info(f"ğŸ² è¨­ç½®éš¨æ©Ÿç¨®å­: {args.seed}")
    
    try:
        # åˆå§‹åŒ–è©•ä¼°å™¨
        evaluator = RetrievalEvaluator(
            db_name=args.db_name, 
            embedding_model=args.embedding_model,
            max_concurrent=args.max_concurrent,
            use_rerank=args.rerank,
            reranker_api_url=args.reranker_api_url,
            reranker_model=args.reranker_model,
            rerank_top_n=args.rerank_top_n
        )
        
        # åŠ è¼‰æ¸¬è©¦æ•¸æ“š
        data = evaluator.load_test_data(args.json_file)
        
        # é¸æ“‡éš¨æ©ŸæŸ¥è©¢
        queries = evaluator.select_random_queries(data, args.num_queries)
        
        # é¡¯ç¤ºæŸ¥è©¢æ¨¡å¼
        if args.use_keywords:
            logging.info(f"ğŸ”‘ å•Ÿç”¨é—œéµè©æª¢ç´¢æ¨¡å¼")
        elif args.use_pseudo_doc:
            logging.info(f"ğŸ”„ å•Ÿç”¨Query2Docå¯¦é©—æ¨¡å¼")
        else:
            logging.info(f"ğŸ“ ä½¿ç”¨æ¨™æº–æŸ¥è©¢æ¨¡å¼")
        
        # åŸ·è¡Œè©•ä¼°
        results = evaluator.evaluate_queries(queries, args.query_type, args.k, args.save_top_results, args.use_pseudo_doc, args.use_keywords)
        
        # é¡¯ç¤ºçµæœ
        evaluator.print_summary(results)
        
        # ä¿å­˜çµæœ
        output_folder = evaluator.save_results(results, args.output_folder, args.use_pseudo_doc, args.use_keywords)
        
        # å¯è¦–åŒ–åœ–è¡¨
        if output_folder:
            evaluator.visualize_ranking_distribution(results, output_folder)
            evaluator.generate_simple_recall_curve(results, output_folder)
        
    except Exception as e:
        logging.error(f"âŒ ç¨‹åºåŸ·è¡Œå¤±æ•—: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)


if __name__ == "__main__":
    main()
